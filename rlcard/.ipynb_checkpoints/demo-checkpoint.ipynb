{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-adjustment",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pursuant-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import get_device, set_seed, tournament, reorganize, Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-duration",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-journalism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running on the CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ha.agarwal\\Downloads\\tarneeb_enviornment-main\\tarneeb_clone_v01\\rlcard\\rlcard\\agents\\dqn_agent.py:198: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 100, rl-loss: 30.60272216796875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 388, rl-loss: 6.51569604873657266"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import get_device, set_seed, tournament, reorganize, Logger\n",
    "\n",
    "\n",
    "# Check whether gpu is available\n",
    "device = get_device()\n",
    "\n",
    "# Seed numpy, torch, random\n",
    "set_seed(7)\n",
    "\n",
    "# Make the environment with seed\n",
    "env = rlcard.make(\"bridge\", config={'seed': 7})\n",
    "\n",
    "# Initialize the agent and use random agents as opponents\n",
    "from rlcard.agents import DQNAgent\n",
    "agent = DQNAgent(num_actions=env.num_actions,\n",
    "                 state_shape=env.state_shape[0],\n",
    "                 mlp_layers=[700 , 700],\n",
    "                 device=device)\n",
    "\n",
    "agents = [agent]\n",
    "for _ in range(env.num_players):\n",
    "    agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "env.set_agents(agents)\n",
    "\n",
    "#for episode in range(2000):\n",
    "\n",
    "for episode in range(3000):\n",
    "    #print('\\n episode:', episode)\n",
    "    trajectories, payoffs = env.run(is_training=True)\n",
    "    #break\n",
    "    # Training\n",
    "    trajectories = reorganize(trajectories, payoffs)\n",
    "    for ts in trajectories[0]:\n",
    "        agent.feed(ts)\n",
    "        #rl_loss = agent.train_rl()\n",
    "        #sl_loss = agent.train_sl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-retention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "institutional-advantage",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "embedded-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "\n",
    "def run_1_simulation():\n",
    "    trajectories, payoffs = env.run()\n",
    "    if payoffs[0] > payoffs[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demonstrated-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.322"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [run_1_simulation() for i in range(2000)] \n",
    "\n",
    "sum(l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compatible-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6779999999999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-cinema",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
