{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-adjustment",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pursuant-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import get_device, set_seed, tournament, reorganize, Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-duration",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sharp-journalism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running on the CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ha.agarwal\\Downloads\\tarneeb_enviornment-main\\tarneeb_clone_v01\\rlcard\\rlcard\\agents\\dqn_agent.py:198: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 100, rl-loss: 30.60272216796875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1100, rl-loss: 10.1023225784301767\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2100, rl-loss: 8.2941398620605476\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3100, rl-loss: 3.4217917919158936\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4100, rl-loss: 9.3518791198730473\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5100, rl-loss: 8.3588294982910165\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6100, rl-loss: 5.4047493934631358\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7100, rl-loss: 5.1528754234313965\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8100, rl-loss: 11.414016723632812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9100, rl-loss: 4.4642047882080085\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10100, rl-loss: 6.8197617530822755\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11100, rl-loss: 6.5070018768310555\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12100, rl-loss: 4.7718586921691895\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13100, rl-loss: 10.101676940917969\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14100, rl-loss: 4.5588235855102546\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 15100, rl-loss: 5.7921843528747564\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 16100, rl-loss: 3.5169916152954165\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 17100, rl-loss: 6.3837122917175296\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 18100, rl-loss: 3.1003155708312995\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 19100, rl-loss: 2.5866117477416993\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 20100, rl-loss: 4.7454614639282237\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 21100, rl-loss: 2.1139178276062012\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 22100, rl-loss: 2.8770744800567627\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 23100, rl-loss: 8.3955059051513675\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 24100, rl-loss: 2.2998287677764893\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 25100, rl-loss: 5.5852789878845215\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 26100, rl-loss: 3.4263472557067876\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 27100, rl-loss: 2.4696156978607178\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 28100, rl-loss: 3.1509802341461184\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 29100, rl-loss: 1.9974273443222046\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 30100, rl-loss: 4.5547266006469732\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 31100, rl-loss: 1.3556761741638184\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 32100, rl-loss: 7.7574777603149412\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 33100, rl-loss: 2.6066291332244873\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 34100, rl-loss: 19.828731536865234\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 35100, rl-loss: 3.5967686176300054\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 36100, rl-loss: 4.8534350395202645\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 37100, rl-loss: 7.3620862960815435\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 38100, rl-loss: 4.5305366516113289\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 39100, rl-loss: 1.9984740018844604\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 40100, rl-loss: 2.6386132240295411\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 41100, rl-loss: 2.5313711166381836\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 42100, rl-loss: 1.8288745880126953\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 43100, rl-loss: 5.0922832489013678\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 43638, rl-loss: 7.2129454612731937"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import get_device, set_seed, tournament, reorganize, Logger\n",
    "\n",
    "\n",
    "# Check whether gpu is available\n",
    "device = get_device()\n",
    "\n",
    "# Seed numpy, torch, random\n",
    "set_seed(7)\n",
    "\n",
    "# Make the environment with seed\n",
    "env = rlcard.make(\"bridge\", config={'seed': 7})\n",
    "\n",
    "# Initialize the agent and use random agents as opponents\n",
    "from rlcard.agents import DQNAgent\n",
    "agent = DQNAgent(num_actions=env.num_actions,\n",
    "                 state_shape=env.state_shape[0],\n",
    "                 mlp_layers=[700 , 700],\n",
    "                 device=device)\n",
    "\n",
    "agents = [agent]\n",
    "for _ in range(env.num_players):\n",
    "    agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "env.set_agents(agents)\n",
    "\n",
    "#for episode in range(2000):\n",
    "\n",
    "for episode in range(3000):\n",
    "    #print('\\n episode:', episode)\n",
    "    trajectories, payoffs = env.run(is_training=True)\n",
    "    #break\n",
    "    # Training\n",
    "    trajectories = reorganize(trajectories, payoffs)\n",
    "    for ts in trajectories[0]:\n",
    "        agent.feed(ts)\n",
    "        #rl_loss = agent.train_rl()\n",
    "        #sl_loss = agent.train_sl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-retention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "institutional-advantage",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "embedded-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "\n",
    "def run_1_simulation():\n",
    "    trajectories, payoffs = env.run()\n",
    "    if payoffs[0] > payoffs[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.277"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [run_1_simulation() for i in range(2000)] \n",
    "\n",
    "sum(l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compatible-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6779999999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-cinema",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
